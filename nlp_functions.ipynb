{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# WORD CLOUD\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "# WORD CLOUD\n",
    "def show_wordcloud(text):\n",
    "    try:\n",
    "        wordcloud= WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(text))\n",
    "        # for wordcloud we pass text not tokens\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        return plt\n",
    "    except Exception as e:\n",
    "        return f\"Error generating word cloud:m {e}\"\n",
    "\n",
    "\n",
    "# N-GRAM ANALYSIS\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_top_ngrams_bar_chart(tokens, gram_n=3, top_n=15):\n",
    "    try:\n",
    "        ngram = list(ngrams(tokens, gram_n))\n",
    "        ngram_counts = Counter(ngram).most_common(top_n)\n",
    "\n",
    "        if not ngram_counts:\n",
    "            raise ValueError(\"No n-grams found in the given  token list\")\n",
    "\n",
    "        labels = []\n",
    "        counts = []\n",
    "        for biagram, count in ngram_counts:\n",
    "            labels.append(\" \".join(biagram))\n",
    "            counts.append(count)\n",
    "\n",
    "        # plotly bar chart\n",
    "        fig = go.Figure(data=\n",
    "        [go.Bar(\n",
    "            x=labels,\n",
    "            y=counts,\n",
    "            text=counts,\n",
    "            textposition=\"outside\")])\n",
    "\n",
    "        # update layout\n",
    "        fig.update_layout(height=550,\n",
    "                          title=\"top 15 Biagrams\",\n",
    "                          xaxis_title=\"Labels\",\n",
    "                          yaxis_title=\"Frequency\")\n",
    "\n",
    "        st.plotly_chart(fig)\n",
    "    except Exception as e:\n",
    "        print(f\"An Error Occured: {e}\")\n",
    "\n",
    "\n",
    "#CREATING CHUNKS\n",
    "import spacy\n",
    "def split_into_chunks_spacy(text, max_length=500):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        sentence = sent.text.strip()\n",
    "        if len(current_chunk) + len(sentence) <= 500:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "#EMOTIONAL ANALYSIS\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "model_name= \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "emotion_classifier = pipeline(\"text-classification\", model=model_name, tokenizer=model_name, top_k=None)\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def detect_emotions(text):\n",
    "    chunks = split_into_chunks_spacy(text)\n",
    "    emotion_totals = {}\n",
    "    emotion_count = {}\n",
    "    emotion_count = defaultdict(int)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        results = emotion_classifier(chunk)[0]\n",
    "        for result in results:\n",
    "            label = result[\"label\"]\n",
    "            score = result[\"score\"]\n",
    "            emotion_totals[label] = emotion_totals.get(label, 0) + score\n",
    "            emotion_count[label] += 1\n",
    "\n",
    "    emotion_counts = dict(emotion_count)\n",
    "\n",
    "    emotion_averages = {label: emotion_totals[label] / emotion_counts[label] for label in emotion_totals}\n",
    "    sorted_emotions = sorted(emotion_averages.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_5 = sorted_emotions[:5]\n",
    "    df = pd.DataFrame(top_5, columns=[\"Emotion\", \"Score\"])\n",
    "    return df\n",
    "\n",
    "# SENTIMENT ANALYSIS\n",
    "\n",
    "model_name= \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "sentiment_classifier= pipeline(\"sentiment-analysis\", model= model_name,  tokenizer= model_name , return_all_scores=True)\n",
    "\n",
    "\n",
    "# CREAING FUNCTION FOR SNETIMENT ANALYSIS\n",
    "def detect_overall_sentiment_avg(text):\n",
    "    try:\n",
    "        sentiment_labels = {\n",
    "            \"LABEL_0\": \"Negative\",\n",
    "            \"LABEL_1\": \"Neutral\",\n",
    "            \"LABEL_2\": \"Positive\"\n",
    "        }\n",
    "        chunks = split_into_chunks_spacy(text)\n",
    "        score_total = {\"Negative\": 0.0, \"Neutral\": 0.0, \"Positive\": 0.0}\n",
    "        chunk_count = len(chunks)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            results = sentiment_classifier(chunk)[0]\n",
    "            for res in results:\n",
    "                label = sentiment_labels[res[\"label\"]]\n",
    "                score_total[label] += res[\"score\"]\n",
    "\n",
    "        avg_score = {}\n",
    "        for label in score_total:\n",
    "            avg_score[label] = score_total[label] / chunk_count\n",
    "        overall_sentiment = max(avg_score, key=avg_score.get)\n",
    "        return {\n",
    "            \"overall_sentiment\": overall_sentiment,\n",
    "            \"average_scores\": avg_score\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {f\"error {e}\"}\n",
    "\n",
    "\n",
    "#TONE OF SPEECH CLASSIFICATION\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "labels = [\n",
    "    \"factual\",\n",
    "    \"opinion\",\n",
    "    \"question\",\n",
    "    \"command\",\n",
    "    \"emotion\",\n",
    "    \"personal experience\",\n",
    "    \"suggestion\",\n",
    "    \"story\",\n",
    "    \"prediction\",\n",
    "    \"warning\",\n",
    "    \"instruction\",\n",
    "    \"definition\",\n",
    "    \"narrative\",\n",
    "    \"news\",\n",
    "    \"argument\"\n",
    "]\n",
    "def classify_custom(text):\n",
    "    result = classifier(text, candidate_labels=labels)\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"predicted_category\": result[\"labels\"][0],\n",
    "        \"score\": result[\"scores\"][0],\n",
    "        \"all_categories\": list(zip(result[\"labels\"], result[\"scores\"]))\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# TEXT SUMMARIZATION\n",
    "\n",
    "def summarize_large_text(text):\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "    # Step 1: Split text into manageable chunks\n",
    "    chunks = split_into_chunks_spacy(text, max_length=500)  # Use pysbd or whatever you prefer\n",
    "\n",
    "    # Step 2: Summarize each chunk individually\n",
    "    chunk_summaries = []\n",
    "    for chunk in chunks:\n",
    "        input_length = len(chunk.split())  # rough word count\n",
    "        max_summary_length = min(300, max(30, int(input_length * 0.7)))  # max 70% of input or cap at 300\n",
    "        min_summary_length = min(100, max(20, int(input_length * 0.3)))  # min 30% of input or cap at 100\n",
    "        summary = summarizer(chunk, max_length=max_summary_length, min_length=max_summary_length, do_sample=False)[0]['summary_text']\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    # Step 3: Combine all chunk summaries into one text\n",
    "    combined_summary_text = \" \".join(chunk_summaries)\n",
    "\n",
    "    # Optional Step 4: Summarize the combined summary for a concise result\n",
    "    input_length = len(combined_summary_text.split())  # rough word count\n",
    "    max_summary_length = int(input_length * 0.9)  # max 70% of input or cap at 300\n",
    "    min_summary_length = int(input_length * 0.3)  # min 30% of input or cap at 100\n",
    "    final_summary = summarizer(combined_summary_text, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    return final_summary\n",
    "\n"
   ],
   "id": "8416a232fb7e52fd"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
